{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.28/04\n"
     ]
    }
   ],
   "source": [
    "# %env PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "\n",
    "from bins import Bins\n",
    "from utils import calcAllChi2, calcOneChi2, HistMaker, diffHist\n",
    "from ROOT import TFile, TH1, TH3F, TF2, TCanvas, TStyle, gStyle, Form, Fit\n",
    "import scipy.optimize as opt\n",
    "import math\n",
    "import calendar, os\n",
    "from time import time, gmtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_saved = True\n",
    "range_used = range(0,12)\n",
    "\n",
    "if read_saved:\n",
    "    DIR_NAME = \"results_sameevt_nu15_noise1em1\"\n",
    "else:\n",
    "    current_GMT =   gmtime()\n",
    "    time_stamp = calendar.timegm(current_GMT)\n",
    "    DIR_NAME = f'results_{time_stamp}'\n",
    "    os.mkdir(DIR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = Bins.readFrom(\"ranges.yml\")\n",
    "#histMakerData = HistMaker(\"apr12_diele_088_090_ag123ag_2500A_accepted_np_mix_hc_sample_2.dat\", \"_data\", bins)\n",
    "histMakerData = HistMaker(\"088_090_ag123ag_2500A_accepted_np.dat\", \"_data\", bins)\n",
    "histsData = histMakerData.makeHists()\n",
    "histMakerMC_rho = HistMaker(\"mar19_diele_inmedium_heli0cm_np.dat\", \"_MC\", bins)\n",
    "histMakerMC_pi0 = HistMaker(\"mar19_diele_pi0_heli0cm_np.dat\", \"_MC\", bins)\n",
    "def getHistMakerMC(HIST_INDEX):\n",
    "    return histMakerMC_pi0 if HIST_INDEX < 3 else histMakerMC_rho\n",
    "   # return histMakerMC_rho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_hists_pi0 = histMakerMC_pi0.builder.base_hists\n",
    "can3 = TCanvas(\"can3\",\"can3\",900,300)\n",
    "can3.Divide(3,1)\n",
    "can3.Draw()\n",
    "for i, hist in enumerate(base_hists_pi0[0]):\n",
    "    if i < 3:\n",
    "        can3.cd(i+1)\n",
    "        if not isinstance(hist, list):\n",
    "            hist.GetXaxis().SetTitle(\"cos(#theta_{e}^{#gamma*})\")\n",
    "            hist.GetYaxis().SetTitle(\"#phi_{e}^{#gamma*}\")\n",
    "            hist.Draw(\"COLZ\")\n",
    "        else:\n",
    "            print(hist)\n",
    "can3.SaveAs(f\"{DIR_NAME}/base_hists_pi0_MC.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_hists_rho = histMakerMC_rho.builder.base_hists\n",
    "can5 = TCanvas(\"can5\",\"can5\",900,1200)\n",
    "can5.Divide(3,4)\n",
    "can5.Draw()\n",
    "for i, hist in enumerate(base_hists_rho[0]):\n",
    "    can5.cd(i+1)\n",
    "    if not isinstance(hist, list):\n",
    "        hist.GetXaxis().SetTitle(\"cos(#theta_{e}^{#gamma*})\")\n",
    "        hist.GetYaxis().SetTitle(\"#phi_{e}^{#gamma*}\")\n",
    "        hist.Draw(\"COLZ\")\n",
    "    else:\n",
    "        print(hist)\n",
    "can5.SaveAs(f'{DIR_NAME}/base_hists_rho_MC.gif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can4 = TCanvas(\"can4\",\"can4\",900,1200)\n",
    "can4.Divide(3,4)\n",
    "can4.Draw()\n",
    "for i, hist in enumerate(histsData[0]):\n",
    "    can4.cd(i+1)\n",
    "    if not isinstance(hist, list):\n",
    "        hist.GetXaxis().SetTitle(\"cos(#theta_{e}^{#gamma*})\")\n",
    "        hist.GetYaxis().SetTitle(\"#phi_{e}^{#gamma*}\")\n",
    "        hist.Draw(\"COLZ\")\n",
    "    else:\n",
    "        print(hist)\n",
    "\n",
    "can4.SaveAs(f'{DIR_NAME}/histsData.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.device(\"mps\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import gpytorch\n",
    "import botorch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"bmh\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TH1.SetDefaultSumw2\n",
    "\n",
    "gpytorch.settings.fast_pred_var()\n",
    "gpytorch.settings.fast_pred_samples()\n",
    "\n",
    "N_PARAMS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.mps.is_available()\n",
    "#mps_device = torch.device(\"mps\")\n",
    "mps_device = torch.device(\"cpu\")\n",
    "bounds = torch.tensor([[0, 0,         0],\n",
    "                       [1, 2*math.pi, 2]], dtype=torch.float)\n",
    "bounds = bounds.to(mps_device)\n",
    "\n",
    "\n",
    "grid_r   = torch.linspace(bounds[0][0], bounds[1][0], 101)\n",
    "grid_phi = torch.linspace(bounds[0][1], bounds[1][1], 101)\n",
    "grid_z   = torch.linspace(bounds[0][2], bounds[1][2], 101)\n",
    "\n",
    "grid_x1, grid_x2, grid_x3 = torch.meshgrid(grid_r, grid_phi, grid_z, indexing=\"ij\")\n",
    "\n",
    "xs = torch.vstack([grid_x1.flatten(), grid_x2.flatten(), grid_x3.flatten()]).transpose(-2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPModel(gpytorch.models.ExactGP, botorch.models.gpytorch.GPyTorchModel):\n",
    "# class GPModel(gpytorch.models.ApproximateGP, botorch.models.gpytorch.GPyTorchModel):\n",
    "    _num_outputs = 1\n",
    "\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        mean_x = mean_x.to(mps_device)\n",
    "        covar_x = covar_x.to(mps_device)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def fit_gp_model(train_x, train_y, num_train_iters=500):\n",
    "    train_x = train_x.to(mps_device)\n",
    "    train_y = train_y.to(mps_device)\n",
    "\n",
    "    # declare the GP\n",
    "    noise = 1e-4\n",
    "\n",
    "   # likelihood = gpytorch.likelihoods.StudentTLikelihood()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPModel(train_x, train_y, likelihood)\n",
    "    model.likelihood.noise = noise\n",
    "    likelihood = likelihood.to(mps_device)\n",
    "    model = model.to(mps_device)\n",
    "\n",
    "    # train the hyperparameter (the constant)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    for i in range(num_train_iters):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    return model.cpu(), likelihood.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Come back to standard version\n",
    "#num_queries = 75\n",
    "num_queries = 200\n",
    "num_repeats = 1\n",
    "num_samples = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_errors(xx, dxx):\n",
    "    r, phi, z = xx[0], xx[1], xx[2]\n",
    "    dr, dphi, dz = dxx[0], dxx[1], dxx[2]\n",
    "\n",
    "    dlambda_theta_2 =     (dr * math.cos(phi)) ** 2 + (dphi * r * math.sin(phi)) ** 2 + (dz / 2) ** 2\n",
    "    dlambda_phi_2   =     (dr * math.cos(phi) / 2) ** 2 + (dphi * r * math.sin(phi) / 2) ** 2 + (dz * 3 / 4) ** 2\n",
    "    dlambda_theta_phi_2 = 0.5 * (dr * math.sin(phi)) ** 2 + 0.5 * (dphi * r * math.cos(phi)) ** 2\n",
    "\n",
    "    dlambda_theta = math.sqrt(dlambda_theta_2)\n",
    "    dlambda_phi = math.sqrt(dlambda_phi_2)\n",
    "    dlambda_theta_phi = math.sqrt(dlambda_theta_phi_2)\n",
    "    return dlambda_theta, dlambda_phi, dlambda_theta_phi\n",
    "\n",
    "def lambdas(xx):\n",
    "   # return x[0], x[1], x[2]\n",
    "    r, phi, z = xx[0], xx[1], xx[2]\n",
    "    x = r*math.cos(phi)\n",
    "    y = r*math.sin(phi)\n",
    "    lambda_theta = 0.5 * (2*x + z)\n",
    "    lambda_phi   = 0.25 * (-2 - 2*x + 3*z)\n",
    "    lambda_theta_phi = y/math.sqrt(2.)\n",
    "    return lambda_theta, lambda_phi, lambda_theta_phi\n",
    "\n",
    "def all_lambdas(xx):\n",
    "    def generator(xx):\n",
    "        for x in xx:\n",
    "            lambda_theta, lambda_phi, lambda_theta_phi = lambdas(x)\n",
    "            yield torch.tensor([lambda_theta, lambda_phi, lambda_theta_phi])\n",
    "           # yield torch.tensor([1./(chi2 / ndf)])\n",
    "    return torch.stack([a for a in generator(xx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -2 is the default value when no feasible has been found\n",
    "default_value = -1\n",
    "\n",
    "def fit_bo(HIST_INDEX = 0):\n",
    "    global bounds\n",
    "\n",
    "    def objective(xx):\n",
    "        def generator(xx):\n",
    "            for x in xx:\n",
    "                lambda_theta, lambda_phi, lambda_theta_phi = lambdas(x)\n",
    "\n",
    "                histsMC = getHistMakerMC(HIST_INDEX).makeHists(lambda_theta, lambda_phi, lambda_theta_phi)\n",
    "                chi2, ndf = calcOneChi2(histsMC[0][HIST_INDEX], histsData[0][HIST_INDEX])\n",
    "                allHistsMC.append(histsMC[0][HIST_INDEX])\n",
    "                if not chi2 or not ndf:\n",
    "                    return torch.tensor([0])\n",
    "                yield torch.tensor([1.0/(chi2 / ndf)])\n",
    "        return torch.stack([a for a in generator(xx)])\n",
    "\n",
    "    def one_starting_sample():\n",
    "        result = bounds[0] + (bounds[1] - bounds[0]) * torch.rand(3)\n",
    "        while -2.0*result[0] - 1.0*result[2] < -2:\n",
    "            result = bounds[0] + (bounds[1] - bounds[0]) * torch.rand(3)\n",
    "        return result\n",
    "\n",
    "    feasible_incumbents = torch.ones((num_repeats, num_queries)) * default_value\n",
    "\n",
    "    best_fs = []\n",
    "\n",
    "    print(f\"HIST INDEX: {HIST_INDEX}\")\n",
    "    for trial in range(num_repeats):\n",
    "       # print(\"trial\", trial)\n",
    "\n",
    "        torch.manual_seed(trial)\n",
    "       # train_x = bounds[0] + (bounds[1] - bounds[0]) * torch.rand(num_samples, 3)\n",
    "       # while -2.0*train_x[0][0] - 1.0*train_x[0][2] < -2:\n",
    "       #     train_x = bounds[0] + (bounds[1] - bounds[0]) * torch.rand(num_samples, 3)\n",
    "        train_x = torch.stack([one_starting_sample() for _ in range(num_samples)])\n",
    "        train_utility = objective(train_x).to(mps_device)\n",
    "        train_x = train_x.to(mps_device)\n",
    "\n",
    "        for i in tqdm(range(num_queries)):\n",
    "            feasible_incumbents[trial, i] = train_utility.max()\n",
    "            before_fit_gp = time()\n",
    "            utility_model, utility_likelihood = fit_gp_model(\n",
    "                train_x, train_utility.squeeze(-1)\n",
    "            )\n",
    "            after_fit_gp = time()\n",
    "            print(\"Fitting GP took \" + str(after_fit_gp - before_fit_gp) + \" seconds\")\n",
    "            best_f = train_utility.max()\n",
    "            best_fs.append(best_f.item())\n",
    "                \n",
    "           # policy = botorch.acquisition.monte_carlo.qExpectedImprovement(\n",
    "           # policy = botorch.acquisition.analytic.LogExpectedImprovement(\n",
    "            policy = botorch.acquisition.logei.qLogExpectedImprovement(\n",
    "          # policy = botorch.acquisition.analytic.LogProbabilityOfImprovement(\n",
    "          # policy = botorch.acquisition.analytic.PosteriorMean(\n",
    "                model=utility_model,\n",
    "                best_f=train_utility.max(),\n",
    "            ).to(mps_device)\n",
    "\n",
    "            before_optimize_acqf = time()\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "                next_x, acq_val = botorch.optim.optimize_acqf(\n",
    "                    policy,\n",
    "                    bounds=bounds,\n",
    "                    #q=5,\n",
    "                    #num_restarts=5,\n",
    "                    #raw_samples=400,\n",
    "                    q=1,\n",
    "                    num_restarts=40,\n",
    "                    raw_samples=100,\n",
    "                    inequality_constraints=[(torch.tensor([0,2],device=mps_device),torch.tensor([-2.0,-1.0],dtype=torch.float,device=mps_device),-2.0)],\n",
    "                )\n",
    "            after_optimize_acqf = time()\n",
    "            print(\"Optimizing ACQF took \" + str(after_optimize_acqf - before_optimize_acqf) + \" seconds\")\n",
    "\n",
    "            next_utility = objective(next_x).to(mps_device)\n",
    "\n",
    "            train_x = torch.cat([train_x, next_x])\n",
    "            train_utility = torch.cat([train_utility, next_utility])\n",
    "    torch.save(feasible_incumbents, f\"{DIR_NAME}/incumbents_\" + str(HIST_INDEX) + \".pth\")\n",
    "    fout = TFile(f\"{DIR_NAME}/out_{HIST_INDEX}.root\",\"RECREATE\")\n",
    "    fout.cd()\n",
    "    for hist in allHistsMC:\n",
    "       # print (\"Writing hist: \", hist.GetName())\n",
    "        hist.Write()\n",
    "    for j, hists in enumerate(histsData):\n",
    "        for k, hist in enumerate(hists):\n",
    "                hist.Write()\n",
    "               # print (\"Writing hist: \", j, k, hist)\n",
    "    fout.Close()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictive_distribution = utility_likelihood(utility_model(xs))\n",
    "        acquisition_score = policy(xs.unsqueeze(1))\n",
    "\n",
    "    lds = all_lambdas(train_x)\n",
    "    lambda_thetas     = lds.transpose(-2,-1)[0]\n",
    "    lambda_phis       = lds.transpose(-2,-1)[1]\n",
    "    lambda_theta_phis = lds.transpose(-2,-1)[2]\n",
    "    c = torch.stack((\n",
    "        torch.arange(0,num_queries, dtype=int),\n",
    "        lambda_thetas[0:num_queries],\n",
    "        lambda_phis[0:num_queries],\n",
    "        lambda_theta_phis[0:num_queries],\n",
    "        train_utility.squeeze()[0:num_queries],\n",
    "        (feasible_incumbents==feasible_incumbents.max())[0],\n",
    "        feasible_incumbents[0]\n",
    "    ),0).transpose(-2,-1)\n",
    "    torch.set_printoptions(precision=4,threshold=10_000, linewidth=120)\n",
    "    sort_index = c[:, 4].sort()[1]\n",
    "    c_sorted = c[sort_index]\n",
    "    train_x_sorted = train_x[sort_index]\n",
    "\n",
    "    return c_sorted, train_x_sorted, predictive_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_sorted_all = []\n",
    "train_x_sorted_all = []\n",
    "predictive_mean_all = []\n",
    "predictive_lower_all = []\n",
    "predictive_upper_all = []\n",
    "\n",
    "for HIST_INDEX in range_used:\n",
    "    allHistsMC = []\n",
    "    if read_saved:\n",
    "        c_sorted = torch.load(f\"{DIR_NAME}/c_sorted_\" + str(HIST_INDEX) + \".pth\")\n",
    "        train_x_sorted = torch.load(f\"{DIR_NAME}/train_x_sorted_\" + str(HIST_INDEX) + \".pth\")\n",
    "        predictive_distribution = torch.load(f\"{DIR_NAME}/predictive_distribution_\" + str(HIST_INDEX) + \".pth\")\n",
    "\n",
    "        file = TFile(f\"{DIR_NAME}/out_{HIST_INDEX}.root\", \"read\")\n",
    "        names = [key.GetName() for key in file.GetListOfKeys()]\n",
    "        names = [name for name in names if \"MC\" in name]\n",
    "        for name in names:\n",
    "            allHistsMC.append(file.Get(name))\n",
    "    else:\n",
    "        c_sorted, train_x_sorted, predictive_distribution = fit_bo(HIST_INDEX)\n",
    "        print(c_sorted[-5:])\n",
    "        \n",
    "        torch.save(c_sorted, f\"{DIR_NAME}/c_sorted_\" + str(HIST_INDEX) + \".pth\")\n",
    "        torch.save(train_x_sorted, f\"{DIR_NAME}/train_x_sorted_\" + str(HIST_INDEX) + \".pth\")\n",
    "        torch.save(predictive_distribution, f\"{DIR_NAME}/predictive_distribution_\" + str(HIST_INDEX) + \".pth\")\n",
    "\n",
    "    predictive_mean = predictive_distribution.mean\n",
    "    predictive_lower, predictive_upper = predictive_distribution.confidence_region()\n",
    "    print(\"AFTER_READING: \", predictive_mean[101], predictive_lower[101], predictive_upper[101])\n",
    "\n",
    "    c_sorted_all.append(c_sorted)\n",
    "    train_x_sorted_all.append(train_x_sorted)\n",
    "    predictive_mean_all.append(predictive_mean)\n",
    "    predictive_lower_all.append(predictive_lower)\n",
    "    predictive_upper_all.append(predictive_upper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, Layout, IntSlider, FloatSlider\n",
    "import numpy as np\n",
    "\n",
    "axis_titles = [r\"R of cone coordinates\", r\"$\\Phi$ of cone coordinates\", r\"Z of cone coordinates\"]\n",
    "\n",
    "def make_proj_2d(xs, tensor, transpose, index):\n",
    "        tensor_3d = torch.reshape(tensor, (101,101,101))\n",
    "        xs_3d = torch.reshape(xs, (101,101,101,3))\n",
    "\n",
    "        if transpose:\n",
    "               tensor_3d = tensor_3d.transpose(transpose[0],transpose[1])\n",
    "               xs_3d     = xs_3d    .transpose(transpose[0],transpose[1])\n",
    "        return xs_3d, tensor_3d[index]\n",
    "\n",
    "def make_proj_1d(xs, tensor, transpose1, transpose2, index1, index2):\n",
    "        tensor_3d = torch.reshape(tensor, (101,101,101))\n",
    "        xs_3d = torch.reshape(xs, (101,101,101,3))\n",
    "\n",
    "        if transpose1:\n",
    "               tensor_3d = tensor_3d.transpose(transpose1[0],transpose1[1])\n",
    "               xs_3d     = xs_3d    .transpose(transpose1[0],transpose1[1])\n",
    "        if transpose2:\n",
    "               tensor_3d = tensor_3d.transpose(transpose2[0],transpose2[1])\n",
    "               xs_3d     = xs_3d    .transpose(transpose2[0],transpose2[1])\n",
    "        return xs_3d, tensor_3d[index1][index2]\n",
    "\n",
    "def oneplot(ax, tensor, index, cmap, title, transpose=None, colorbar=False):\n",
    "        \n",
    "        global xs\n",
    "\n",
    "        xs_3d, proj = make_proj_2d(xs, tensor, transpose, index)\n",
    "        if transpose is None:\n",
    "                extent=[\n",
    "                        xs_3d[0][0][0][2],\n",
    "                        xs_3d[0][0][-1][2],\n",
    "                        xs_3d[0][0][0][1],\n",
    "                        xs_3d[0][-1][0][1],\n",
    "                ]\n",
    "        elif transpose == (0,1):\n",
    "                extent=[\n",
    "                        xs_3d[0][0][0][2],\n",
    "                        xs_3d[0][0][-1][2],\n",
    "                        xs_3d[0][0][0][0],\n",
    "                        xs_3d[0][-1][0][0],\n",
    "                ]\n",
    "        elif transpose == (0,2):\n",
    "                extent=[\n",
    "                        xs_3d[0][0][0][0],\n",
    "                        xs_3d[0][0][-1][0],\n",
    "                        xs_3d[0][0][0][1],\n",
    "                        xs_3d[0][-1][0][1],\n",
    "                ]\n",
    "        if transpose is None:\n",
    "                x_title=axis_titles[2]\n",
    "                y_title=axis_titles[1]\n",
    "        elif transpose == (0, 1):\n",
    "                x_title=axis_titles[2]\n",
    "                y_title=axis_titles[0]\n",
    "        elif transpose == (0,2):\n",
    "                x_title=axis_titles[0]\n",
    "                y_title=axis_titles[1]\n",
    "        \n",
    "        pos = ax.imshow(proj, cmap=cmap, interpolation=\"nearest\", origin=\"lower\", \n",
    "                vmin=0, vmax=tensor.max(), extent=extent)\n",
    "        ax.set_aspect((extent[1]-extent[0])/(extent[3]-extent[2]))\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(x_title)\n",
    "        ax.set_ylabel(y_title)\n",
    "        if colorbar:\n",
    "                plt.colorbar(pos,fraction=0.046, pad=0.04)\n",
    "\n",
    "\n",
    "\n",
    "def bin_index(x, scale):\n",
    "        return int(x*100/scale)\n",
    "\n",
    "def ff(ax, x, transpose, scale, colorbar):\n",
    "        cmap = \"gist_rainbow\"\n",
    "        index = bin_index(x, scale)\n",
    "        oneplot(ax, predictive_mean, index, cmap, r\"ndf$/\\chi^2$\", transpose, colorbar=colorbar)\n",
    "       # oneplot(ax[1], predictive_upper-predictive_lower, index, cmap, \"confidence int. width\", transpose)\n",
    "       # oneplot(ax[1], acquisition_score, index, cmap, \"acquisition score\", transpose)\n",
    "        \n",
    "        return ax\n",
    "\n",
    "\n",
    "def f(ax, x):\n",
    "        return ff(ax, x, None, 1.0, colorbar=False)\n",
    "        \n",
    "def g(ax, x):\n",
    "        return ff(ax, x, (0,1),2*math.pi, colorbar=False)\n",
    "\n",
    "def h(ax, x):\n",
    "        return ff(ax, x, (0,2), 2.0, colorbar=True)\n",
    "\n",
    "gStyle.SetOptStat(0)\n",
    "\n",
    "can_cmp_ind = 0\n",
    "\n",
    "def plotComparison(histMC, histData):\n",
    "    print(f\"PLOTTING: {histMC.GetName()} and {histData.GetName()}\")\n",
    "    global can_cmp_ind\n",
    "\n",
    "    can = TCanvas(\"can_cmp\"+str(can_cmp_ind),\"can\",900,600)\n",
    "    can_cmp_ind = can_cmp_ind+1\n",
    "    can.Divide(3,2)\n",
    "    can.Draw()\n",
    "    can.cd(1)\n",
    "    histMC.GetXaxis().SetTitle(\"cos(#theta_{e}^{#gamma*})\")\n",
    "    histMC.GetYaxis().SetTitle(\"#phi_{e}^{#gamma*}\")\n",
    "    histMC.Draw(\"COLZ\")\n",
    "    can.cd(2)\n",
    "    histData.GetXaxis().SetTitle(\"cos(#theta_{e}^{#gamma*})\")\n",
    "    histData.GetYaxis().SetTitle(\"#phi_{e}^{#gamma*}\")\n",
    "    histData.Draw(\"COLZ\")\n",
    "    pad = can.cd(4)\n",
    "    pad.SetRightMargin(0.155)\n",
    "    hdiff = diffHist(histMC, histData)\n",
    "    hdiff.Draw(\"COLZ\")\n",
    "    can.Update()\n",
    "    palette = hdiff.GetListOfFunctions().FindObject(\"palette\")\n",
    "    palette.SetX1NDC(0.85)\n",
    "    palette.SetX2NDC(0.9)\n",
    "    can.Modified()\n",
    "    can.Update()\n",
    "    \n",
    "    can.cd(5)\n",
    "    nbx = histMC.GetNbinsX()\n",
    "    py = histMC.ProjectionY(histMC.GetName()+\"py\",1,nbx)\n",
    "    py.Draw(\"HIST\")\n",
    "    pyData = histData.ProjectionY(\"pyData\",1,nbx)\n",
    "    pyData.SetLineColor(2)\n",
    "    pyData.SetMarkerColor(2)\n",
    "    pyData.Draw(\"SAME\")\n",
    "    \n",
    "    can.cd(6)\n",
    "    nby = histMC.GetNbinsY()\n",
    "    px = histMC.ProjectionX(histMC.GetName()+\"px\",1,nby)\n",
    "    px.Draw(\"HIST\")\n",
    "    pxData = histData.ProjectionX(\"pxData\",1,nby)\n",
    "    pxData.SetLineColor(2)\n",
    "    pxData.SetMarkerColor(2)\n",
    "    pxData.Draw(\"SAME\")\n",
    "    \n",
    "\n",
    "    return can, hdiff, py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_h3d(predictive_mean, HIST_INDEX):\n",
    "    pm = predictive_mean.reshape(101,101,101)\n",
    "    pm_min = pm.min()\n",
    "    hpm = TH3F(\"hpm\",\"predictive mean\", 101, 0, 1, 101, 0, 2*math.pi, 101, 0, 2)\n",
    "    for binx in range(1,101):\n",
    "        for biny in range(1,101):\n",
    "            for binz in range(1,101):\n",
    "                if pm_min < 0:\n",
    "                    hpm.SetBinContent(binx,biny,binz,pm[binx][biny][binz].item()-pm_min)\n",
    "                else:\n",
    "                    hpm.SetBinContent(binx,biny,binz,pm[binx][biny][binz].item())\n",
    "    hpm.SaveAs(f\"{DIR_NAME}/pm_{HIST_INDEX}.root\")\n",
    "    return hpm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, Layout, IntSlider, FloatSlider\n",
    "import numpy as np\n",
    "\n",
    "axis_titles = [r\"R of cone coordinates\", r\"$\\Phi$ of cone coordinates\", r\"Z of cone coordinates\"]\n",
    "\n",
    "def make_proj_2d(xs, tensor, transpose, index):\n",
    "        tensor_3d = torch.reshape(tensor, (101,101,101))\n",
    "        xs_3d = torch.reshape(xs, (101,101,101,3))\n",
    "\n",
    "        if transpose:\n",
    "               tensor_3d = tensor_3d.transpose(transpose[0],transpose[1])\n",
    "               xs_3d     = xs_3d    .transpose(transpose[0],transpose[1])\n",
    "        return xs_3d, tensor_3d[index]\n",
    "\n",
    "def make_proj_1d(xs, tensor, transpose1, transpose2, index1, index2):\n",
    "        tensor_3d = torch.reshape(tensor, (101,101,101))\n",
    "        xs_3d = torch.reshape(xs, (101,101,101,3))\n",
    "\n",
    "        if transpose1:\n",
    "               tensor_3d = tensor_3d.transpose(transpose1[0],transpose1[1])\n",
    "               xs_3d     = xs_3d    .transpose(transpose1[0],transpose1[1])\n",
    "        if transpose2:\n",
    "               tensor_3d = tensor_3d.transpose(transpose2[0],transpose2[1])\n",
    "               xs_3d     = xs_3d    .transpose(transpose2[0],transpose2[1])\n",
    "        return xs_3d, tensor_3d[index1][index2]\n",
    "\n",
    "def oneplot(ax, tensor, index, cmap, title, transpose=None, colorbar=False):\n",
    "        \n",
    "        global xs\n",
    "\n",
    "        xs_3d, proj = make_proj_2d(xs, tensor, transpose, index)\n",
    "        if transpose is None:\n",
    "                extent=[\n",
    "                        xs_3d[0][0][0][2],\n",
    "                        xs_3d[0][0][-1][2],\n",
    "                        xs_3d[0][0][0][1],\n",
    "                        xs_3d[0][-1][0][1],\n",
    "                ]\n",
    "        elif transpose == (0,1):\n",
    "                extent=[\n",
    "                        xs_3d[0][0][0][2],\n",
    "                        xs_3d[0][0][-1][2],\n",
    "                        xs_3d[0][0][0][0],\n",
    "                        xs_3d[0][-1][0][0],\n",
    "                ]\n",
    "        elif transpose == (0,2):\n",
    "                extent=[\n",
    "                        xs_3d[0][0][0][0],\n",
    "                        xs_3d[0][0][-1][0],\n",
    "                        xs_3d[0][0][0][1],\n",
    "                        xs_3d[0][-1][0][1],\n",
    "                ]\n",
    "        if transpose is None:\n",
    "                x_title=axis_titles[2]\n",
    "                y_title=axis_titles[1]\n",
    "        elif transpose == (0, 1):\n",
    "                x_title=axis_titles[2]\n",
    "                y_title=axis_titles[0]\n",
    "        elif transpose == (0,2):\n",
    "                x_title=axis_titles[0]\n",
    "                y_title=axis_titles[1]\n",
    "        \n",
    "        pos = ax.imshow(proj, cmap=cmap, interpolation=\"nearest\", origin=\"lower\", \n",
    "                vmin=0, vmax=tensor.max(), extent=extent)\n",
    "        ax.set_aspect((extent[1]-extent[0])/(extent[3]-extent[2]))\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(x_title)\n",
    "        ax.set_ylabel(y_title)\n",
    "        if colorbar:\n",
    "                plt.colorbar(pos,fraction=0.046, pad=0.04)\n",
    "\n",
    "\n",
    "\n",
    "def bin_index(x, scale):\n",
    "        return int(x*100/scale)\n",
    "\n",
    "def ff(ax, x, values, transpose, scale, colorbar):\n",
    "        cmap = \"gist_rainbow\"\n",
    "        index = bin_index(x, scale)\n",
    "        oneplot(ax, values, index, cmap, r\"ndf$/\\chi^2$\", transpose, colorbar=colorbar)\n",
    "       # oneplot(ax[1], predictive_upper-predictive_lower, index, cmap, \"confidence int. width\", transpose)\n",
    "       # oneplot(ax[1], acquisition_score, index, cmap, \"acquisition score\", transpose)\n",
    "        \n",
    "        return ax\n",
    "\n",
    "\n",
    "def f(ax, x, values):\n",
    "        return ff(ax, x, values, None, 1.0, colorbar=False)\n",
    "        \n",
    "def g(ax, x, values):\n",
    "        return ff(ax, x, values, (0,1),2*math.pi, colorbar=False)\n",
    "\n",
    "def h(ax, x, values):\n",
    "        return ff(ax, x, values, (0,2), 2.0, colorbar=True)\n",
    "\n",
    "gStyle.SetOptStat(0)\n",
    "\n",
    "can_cmp_ind = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance_random_sampling(predictive_mean, HIST_INDEX):\n",
    "    hpm = convert_to_h3d((predictive_mean, HIST_INDEX))\n",
    "\n",
    "    from ctypes import c_double\n",
    "    rnd_points = []\n",
    "\n",
    "    x = c_double(0.0)\n",
    "    y = c_double(0.0)\n",
    "    z = c_double(0.0)\n",
    "    for i in range(80_000_000):\n",
    "        if i % 10000000 == 0:\n",
    "            print(i)\n",
    "        hpm.GetRandom3(x,y,z)\n",
    "        rnd_points.append([x.value,y.value,z.value])\n",
    "\n",
    "    print(x.value,y.value,z.value)\n",
    "    covariance = np.cov(rnd_points, rowvar=False)\n",
    "    return covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance_weight(predictive_mean):\n",
    "    predictive_mean_2 = predictive_mean - predictive_mean.min()\n",
    "\n",
    "    covariance_2 = np.cov(xs, rowvar=False, aweights=predictive_mean_2)\n",
    "    np.sqrt(covariance_2)\n",
    "\n",
    "    return covariance_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfit = TCanvas(\"cfit\",\"cfit\",500,500)\n",
    "#cfit.Draw()\n",
    "#cfit.cd()\n",
    "\n",
    "def covariance_fit(hpm, best, HIST_INDEX):\n",
    "    print(best)\n",
    "    def gaus2d(X, a):\n",
    "        N = a[0]\n",
    "        m_x = a[1]\n",
    "        m_y = a[2]\n",
    "\n",
    "        K_xx = a[3]\n",
    "        K_yy = a[4]\n",
    "\n",
    "        K_xy = a[5]\n",
    "\n",
    "        offset = a[6]\n",
    "\n",
    "        x = X[0] - m_x\n",
    "        y = X[1] - m_y\n",
    "\n",
    "       # print(N, x, y, math.exp(-0.5*( x*x*K_xx + y*y*K_yy + 2*x*y*K_xy ) ) )\n",
    "       \n",
    "        return offset + N * math.exp( -0.5*(x*x*K_xx + y*y*K_yy + 2*x*y*K_xy) )\n",
    "\n",
    "    def fit_2d(axis_ind):\n",
    "        if axis_ind == 0:\n",
    "            axis = hpm.GetXaxis()\n",
    "            proj_option = \"yz\"\n",
    "            proj_X_ind, proj_Y_ind = 2, 1\n",
    "        elif axis_ind == 1:\n",
    "            axis = hpm.GetYaxis()\n",
    "            proj_option = \"zx\"\n",
    "            proj_X_ind, proj_Y_ind = 0, 2\n",
    "        else:\n",
    "            axis = hpm.GetZaxis()\n",
    "            proj_option = \"xy\"\n",
    "            proj_X_ind, proj_Y_ind = 1, 0\n",
    "        bin = axis.FindBin(best[axis_ind])\n",
    "        axis.SetRange(bin, bin)\n",
    "        \n",
    "        proj = hpm.Project3D(proj_option)\n",
    "\n",
    "        proj_lastbin_X = proj.GetXaxis().GetNbins()\n",
    "        proj_max_X = proj.GetXaxis().GetBinUpEdge(proj_lastbin_X)\n",
    "        proj_lastbin_Y = proj.GetYaxis().GetNbins()\n",
    "        proj_max_Y = proj.GetYaxis().GetBinUpEdge(proj_lastbin_Y)\n",
    "        scale_X = 0.2*proj_max_X\n",
    "        scale_Y = 0.2*proj_max_Y\n",
    "\n",
    "        xmin = min( best[proj_X_ind]-scale_X, 0.0)\n",
    "        xmax = min( best[proj_X_ind]+scale_X, proj_max_X)\n",
    "        ymin = min( best[proj_Y_ind]-scale_Y, 0.0)\n",
    "        ymax = min( best[proj_Y_ind]+scale_Y, proj_max_Y)\n",
    "\n",
    "        f2 = TF2(\"f2\", gaus2d, xmin, xmax, ymin, ymax, 6)\n",
    "        f2.SetParameter(0,1)\n",
    "        f2.FixParameter(1,best[proj_X_ind])\n",
    "        f2.FixParameter(2,best[proj_Y_ind])\n",
    "        f2.SetParameter(3,0.2*best[proj_X_ind])\n",
    "        f2.SetParameter(4,0.2*best[proj_Y_ind])\n",
    "        f2.SetParameter(5,0)\n",
    "        f2.SetParameter(6,0)\n",
    "\n",
    "        print(\"type of proj\", type(proj))\n",
    "        print(\"type of f2\", type(f2))\n",
    "        proj.Fit(f2,\"R0\")\n",
    "    \n",
    "        return f2.GetParameter(3), f2.GetParameter(4), f2.GetParameter(5)\n",
    "\n",
    "    params0 = fit_2d(0)\n",
    "    params1 = fit_2d(1)\n",
    "    params2 = fit_2d(2)\n",
    "\n",
    "    params = np.array([params0,params1,params2])\n",
    "    print(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance_fit_scipy(predictive_mean, predictive_lower, predictive_upper, best, HIST_INDEX,ax):\n",
    "   # def gaus1d(x, A, mean_x, sigma_x, offset):\n",
    "    def gaus1d(x, A, mean_x, sigma_x):\n",
    "        x = x - mean_x\n",
    "       # z = offset + A * np.exp( -0.5 * (x/sigma_x) ** 2 )\n",
    "        z = A * np.exp( -0.5 * (x/sigma_x) ** 2 )\n",
    "        return z\n",
    "\n",
    "   # def gaus2d(X, A, mean_x, mean_y, K_xx, K_yy, K_xy, offset):\n",
    "    def gaus2d(X, A, mean_x, mean_y, K_xx, K_yy, K_xy, offset):\n",
    "        (x, y) = X\n",
    "        x = x - mean_x\n",
    "        y = y - mean_y\n",
    "        z = offset + A * np.exp( -0.5*(K_xx * x ** 2 + K_yy * y ** 2 + 2 * K_xy * x * y) )\n",
    "        return z\n",
    "\n",
    "    def gaus3d(X, A, mean_x, mean_y, mean_z, K_xx, K_yy, K_zz, K_xy, K_yz, K_zx, offset):\n",
    "        (x, y, z) = X\n",
    "        x = x - mean_x\n",
    "        y = y - mean_y\n",
    "        z = z - mean_z\n",
    "        z = offset + A * np.exp( -0.5*(K_xx * x ** 2 + K_yy * y ** 2 + K_zz * z ** 2 + \n",
    "            2 * K_xy * x * y + 2 * K_yz * y * z + 2 * K_zx * z * x) )\n",
    "        return z\n",
    "\n",
    "    def gaus2d_fixed_mean(mean_x, mean_y):\n",
    "        def inner(X, A, K_xx, K_yy, K_xy, offset):\n",
    "            (x, y) = X\n",
    "            x = x - mean_x\n",
    "            y = y - mean_y\n",
    "            z = offset + A * np.exp( -0.5*(K_xx * x ** 2 + K_yy * y ** 2 + 2 * K_xy * x * y) )\n",
    "            return z\n",
    "        return inner\n",
    "\n",
    "    def gaus3d_fixed_mean(mean_x, mean_y, mean_z):\n",
    "        def inner(X, A, K_xx, K_yy, K_zz, K_xy, K_yz, K_zx, offset):\n",
    "            (x, y, z) = X\n",
    "            x = x - mean_x\n",
    "            y = y - mean_y\n",
    "            z = z - mean_z\n",
    "            z = offset + A * np.exp( -0.5*(K_xx * x ** 2 + K_yy * y ** 2 + K_zz * z ** 2 + \n",
    "                2 * K_xy * x * y + 2 * K_yz * y * z + 2 * K_zx * z * x) )\n",
    "            return z\n",
    "        return inner\n",
    "\n",
    "    def fit_1d(axis_ind, ax):\n",
    "        if axis_ind == 0:\n",
    "            axis1, axis2 = 1, 2\n",
    "            trasnpose1, transpose2 = (0, 1), (1, 2)\n",
    "        elif axis_ind == 1:\n",
    "            axis1, axis2 = 2, 0\n",
    "            trasnpose1, transpose2 = (0, 1), (0, 2)\n",
    "        else:\n",
    "            axis1, axis2 = 0, 1\n",
    "            trasnpose1, transpose2 = None, None\n",
    "\n",
    "        mean_x = best[axis_ind].item()\n",
    "\n",
    "        scale1 = bounds[1][axis1].item()\n",
    "        scale2 = bounds[1][axis2].item()\n",
    "        proj_max_X = bounds[1][axis_ind].item()\n",
    "\n",
    "        index1 = bin_index(train_x_sorted[-1][axis1], scale1)\n",
    "        index2 = bin_index(train_x_sorted[-1][axis2], scale2)\n",
    "        _, proj = make_proj_1d(xs, predictive_mean, trasnpose1, transpose2, index1, index2)\n",
    "        _, proj_lower = make_proj_1d(xs, predictive_lower, trasnpose1, transpose2, index1, index2)\n",
    "        _, proj_upper = make_proj_1d(xs, predictive_upper, trasnpose1, transpose2, index1, index2)\n",
    "\n",
    "        xmin_ind = max(0,   bin_index(mean_x, proj_max_X)-30)\n",
    "        xmax_ind = min(100, bin_index(mean_x, proj_max_X)+30)\n",
    "        xmin = xmin_ind / 100. * proj_max_X\n",
    "        xmax = (xmax_ind + 1) / 100. * proj_max_X\n",
    "\n",
    "        x = np.linspace(0, proj_max_X, 101)\n",
    "       # fig, ax = plt.subplots(nrows=1, ncols=1, sharey=False)\n",
    "\n",
    "        proj1 = proj[xmin_ind:xmax_ind]\n",
    "    \n",
    "       # initial_guess = (1.0, mean_x, 0.2*proj_max_X, 0.0)\n",
    "        initial_guess = (1.0, mean_x, 0.2*proj_max_X)\n",
    "\n",
    "        popt, pcov = opt.curve_fit(gaus1d, x[xmin_ind:xmax_ind], proj1, p0 = initial_guess)\n",
    "        fit_result = gaus1d(x, *(popt))\n",
    "       # if axis_ind == 2:\n",
    "       # if axis_ind == 0 or axis_ind == 2:\n",
    "       # if False:\n",
    "        if True:\n",
    "        \n",
    "            ax.fill_between(x, proj_lower, proj_upper, alpha=0.5)\n",
    "            ax.plot(x, proj, label=\"Estimated values\")\n",
    "            ax.set_xlabel(axis_titles[axis_ind])\n",
    "            ax.set_ylabel(r\"ndf$/\\chi^2$\")\n",
    "            ax.plot(x, fit_result, label=\"Gaussian fit\")\n",
    "            ax.set_ylim([0,1.5*predictive_upper.max()])\n",
    "            ax.legend()\n",
    "        return popt, pcov\n",
    "\n",
    "\n",
    "    def fit_2d(axis_ind):\n",
    "        if axis_ind == 0:\n",
    "            trasnpose = None\n",
    "            proj_X_ind, proj_Y_ind = 2, 1\n",
    "        elif axis_ind == 1:\n",
    "            trasnpose = (0, 1)\n",
    "            proj_X_ind, proj_Y_ind = 0, 2\n",
    "        else:\n",
    "            trasnpose = (0, 2)\n",
    "            proj_X_ind, proj_Y_ind = 0, 1\n",
    "\n",
    "        mean_x = best[proj_X_ind].item()\n",
    "        mean_y = best[proj_Y_ind].item()\n",
    "\n",
    "        scale = bounds[1][axis_ind].item()\n",
    "        proj_max_X = bounds[1][proj_X_ind].item()\n",
    "        proj_max_Y = bounds[1][proj_Y_ind].item()\n",
    "\n",
    "        index = bin_index(train_x_sorted[-1][axis_ind], scale)\n",
    "        _, proj = make_proj_2d(xs, predictive_mean, trasnpose, index)\n",
    "\n",
    "        xmin_ind = max(0,   bin_index(mean_x, proj_max_X)-30)\n",
    "        xmax_ind = min(100, bin_index(mean_x, proj_max_X)+30)\n",
    "        ymin_ind = max(0,   bin_index(mean_y, proj_max_Y)-30)\n",
    "        ymax_ind = min(100, bin_index(mean_y, proj_max_Y)+30)\n",
    "        xmin = xmin_ind / 100. * proj_max_X\n",
    "        xmax = (xmax_ind + 1) / 100. * proj_max_X\n",
    "        ymin = ymin_ind / 100. * proj_max_Y\n",
    "        ymax = (ymax_ind + 1) / 100. * proj_max_Y\n",
    "\n",
    "       # fig, ax = plt.subplots(nrows=1, ncols=3, sharey=False, constrained_layout=True)\n",
    "        ax[0].imshow(proj, interpolation=\"nearest\", origin=\"lower\", cmap=\"gist_rainbow\",\n",
    "            aspect=proj_max_X/proj_max_Y,\n",
    "            vmin=0, vmax=predictive_mean.max(), extent=(0,proj_max_X,0,proj_max_Y))\n",
    "        proj1 = proj[ymin_ind:ymax_ind, xmin_ind:xmax_ind]\n",
    "\n",
    "       # initial_guess = (1.0, mean_x, mean_y, 0.2*proj_max_X, 0.2*proj_max_Y, 0.0, 0.0)\n",
    "        initial_guess = (1.0, 0.2*proj_max_X, 0.2*proj_max_Y, 0.0, 0.0)\n",
    "    \n",
    "        x = np.linspace(xmin, xmax, xmax_ind-xmin_ind)\n",
    "        y = np.linspace(ymin, ymax, ymax_ind-ymin_ind)\n",
    "        x, y = np.meshgrid(x, y)\n",
    "        x = x.flatten()\n",
    "        y = y.flatten()\n",
    "\n",
    "        ax[1].imshow(proj1, interpolation=\"nearest\", origin=\"lower\", cmap=\"gist_rainbow\",\n",
    "            aspect=(xmax-xmin)/(ymax-ymin),\n",
    "            vmin=0, vmax=predictive_mean.max(), extent=(xmin,xmax,ymin,ymax))\n",
    "        popt, pcov = opt.curve_fit(gaus2d_fixed_mean(mean_x, mean_y), (x,y), proj1.ravel(), p0 = initial_guess)\n",
    "        fit_result = gaus2d_fixed_mean(mean_x, mean_y)((x,y), *(popt)).reshape(ymax_ind-ymin_ind,xmax_ind-xmin_ind)\n",
    "        ax[2].imshow(fit_result, interpolation=\"nearest\", origin=\"lower\", cmap=\"gist_rainbow\",\n",
    "            aspect=(xmax-xmin)/(ymax-ymin),\n",
    "            vmin=0, vmax=predictive_mean.max(), extent=(xmin,xmax,ymin,ymax))\n",
    "        return popt, pcov\n",
    "\n",
    "    def fit_3d():\n",
    "        mean_x = best[0].item()\n",
    "        mean_y = best[1].item()\n",
    "        mean_z = best[2].item()\n",
    "        \n",
    "        proj_max_X = bounds[1][0].item()\n",
    "        proj_max_Y = bounds[1][1].item()\n",
    "        proj_max_Z = bounds[1][2].item()\n",
    "\n",
    "        xmin_ind = max(0,   bin_index(mean_x, proj_max_X)-30)\n",
    "        xmax_ind = min(100, bin_index(mean_x, proj_max_X)+30)\n",
    "        ymin_ind = max(0,   bin_index(mean_y, proj_max_Y)-30)\n",
    "        ymax_ind = min(100, bin_index(mean_y, proj_max_Y)+30)\n",
    "        zmin_ind = max(0,   bin_index(mean_z, proj_max_Z)-30)\n",
    "        zmax_ind = min(100, bin_index(mean_z, proj_max_Z)+30)\n",
    "        xmin = xmin_ind / 100. * proj_max_X\n",
    "        xmax = (xmax_ind + 1) / 100. * proj_max_X\n",
    "        ymin = ymin_ind / 100. * proj_max_Y\n",
    "        ymax = (ymax_ind + 1) / 100. * proj_max_Y\n",
    "        zmin = zmin_ind / 100. * proj_max_Z\n",
    "        zmax = (zmax_ind + 1) / 100. * proj_max_Z\n",
    "\n",
    "        predictive_mean1 = predictive_mean.reshape(101,101,101)\n",
    "        predictive_mean1 = predictive_mean1[xmin_ind:xmax_ind,ymin_ind:ymax_ind,zmin_ind:zmax_ind]\n",
    "\n",
    "       # initial_guess = (1.0, mean_x, mean_y, mean_z, 0.2*proj_max_X, 0.2*proj_max_Y, 0.2*proj_max_Z, 0.0, 0.0, 0.0, 0.0)\n",
    "        initial_guess = (1.0, 0.2*proj_max_X, 0.2*proj_max_Y, 0.2*proj_max_Z, 0.0, 0.0, 0.0, 0.0)\n",
    "    \n",
    "        x = np.linspace(xmin, xmax, xmax_ind-xmin_ind)\n",
    "        y = np.linspace(ymin, ymax, ymax_ind-ymin_ind)\n",
    "        z = np.linspace(zmin, zmax, zmax_ind-zmin_ind)\n",
    "        x, y, z = np.meshgrid(x, y, z)\n",
    "        x = x.flatten()\n",
    "        y = y.flatten()\n",
    "        z = z.flatten()\n",
    "\n",
    "        popt, pcov = opt.curve_fit(gaus3d_fixed_mean(mean_x, mean_y, mean_z), (x,y,z), predictive_mean1.ravel(), p0 = initial_guess)\n",
    "        return popt, pcov\n",
    "\n",
    "   # fig, ax = plt.subplots(nrows=1, ncols=3, sharey=False, constrained_layout=True)\n",
    "    params0, _ = fit_1d(0, ax[1][0])\n",
    "   # print(\"params0: \", params0)\n",
    "    params1, _ = fit_1d(1, ax[1][1])\n",
    "   # print(\"params1: \", params1)\n",
    "    params2, _ = fit_1d(2, ax[1][2])\n",
    "   # print(\"params2: \", params2)\n",
    "    plt.savefig(f\"{DIR_NAME}/chi2_best_{HIST_INDEX}.png\", bbox_inches=\"tight\")\n",
    "\n",
    "    return params0[2], params1[2], params2[2]\n",
    "\n",
    "\n",
    "### A, mean_x, mean_y, mean_z, K_xx, K_yy, K_zz, K_xy, K_yz, K_zx, offset\n",
    "   # params, _ = fit_3d()\n",
    "\n",
    "   # print(\"PARAMS: \", params)\n",
    "   # PARAM_OFFSET=-3\n",
    "\n",
    "   # K_RR = params[4+PARAM_OFFSET]\n",
    "   # K_PhiPhi = params[5+PARAM_OFFSET]\n",
    "   # K_ZZ = params[6+PARAM_OFFSET]    \n",
    "   # K_ZPhi = params[7+PARAM_OFFSET]\n",
    "   # K_ZR = params[8+PARAM_OFFSET]\n",
    "   # K_RPhi = params[9+PARAM_OFFSET]\n",
    "\n",
    "   # K = np.array([\n",
    "   #     [K_RR,   K_RPhi,   K_ZR],\n",
    "   #     [K_RPhi, K_PhiPhi, K_ZPhi],\n",
    "   #     [K_ZR,   K_ZPhi,   K_ZZ]\n",
    "   # ])\n",
    "\n",
    "   # print(\"K 1: \", K)\n",
    "   # Sigma = np.linalg.inv(K)\n",
    "   # print(\"COVARIANCE 1: \", Sigma)\n",
    "\n",
    "   # PARAM_OFFSET=-2\n",
    "   \n",
    "   # params0, _ = fit_2d(0)\n",
    "   # print(\"params0: \", params0[3+PARAM_OFFSET:6+PARAM_OFFSET])\n",
    "   # params1, _ = fit_2d(1)\n",
    "   # print(\"params1: \", params1[3+PARAM_OFFSET:6+PARAM_OFFSET])\n",
    "   # params2, _ = fit_2d(2)\n",
    "   # print(\"params2: \", params2[3+PARAM_OFFSET:6+PARAM_OFFSET])\n",
    "\n",
    "   # K_RR_a, K_RR_b = params2[3+PARAM_OFFSET], params1[4+PARAM_OFFSET]\n",
    "   # K_PhiPhi_a, K_PhiPhi_b = params0[4+PARAM_OFFSET], params2[4+PARAM_OFFSET]\n",
    "   # K_ZZ_a, K_ZZ_b = params0[3+PARAM_OFFSET], params1[3+PARAM_OFFSET]\n",
    "   # K_RR = 0.5*(K_RR_a + K_RR_b)\n",
    "   # K_PhiPhi = 0.5*(K_PhiPhi_a + K_PhiPhi_b)\n",
    "   # K_ZZ = 0.5*(K_ZZ_a + K_ZZ_b)\n",
    "   # K_ZPhi = params0[5+PARAM_OFFSET]\n",
    "   # K_ZR = params1[5+PARAM_OFFSET]\n",
    "   # K_RPhi = params2[5+PARAM_OFFSET]\n",
    "\n",
    "   # K = np.array([\n",
    "   #     [K_RR,   K_RPhi,   K_ZR],\n",
    "   #     [K_RPhi, K_PhiPhi, K_ZPhi],\n",
    "   #     [K_ZR,   K_ZPhi,   K_ZZ]\n",
    "   # ])\n",
    "   # print(\"K 2: \", K)\n",
    "   # Sigma = np.linalg.inv(K)\n",
    "   # print(\"COVARIANCE 2: \", Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'{DIR_NAME}/results.txt', 'w') as fout:\n",
    "    for HIST_INDEX in range_used:\n",
    "\n",
    "        c_sorted = c_sorted_all[HIST_INDEX - range_used.start]\n",
    "        train_x_sorted = train_x_sorted_all[HIST_INDEX - range_used.start]\n",
    "        predictive_mean = predictive_mean_all[HIST_INDEX - range_used.start]\n",
    "        predictive_lower = predictive_lower_all[HIST_INDEX - range_used.start]\n",
    "        predictive_upper = predictive_upper_all[HIST_INDEX - range_used.start]\n",
    "        predictive_mean_2 = predictive_mean - predictive_mean.min()\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "        fig.tight_layout(w_pad=2.0, h_pad=4.0)\n",
    "       # convert_to_h3d(predictive_mean, HIST_INDEX)\n",
    "        #interact(f, x=FloatSlider(description=axis_titles[0], min=0.0, max=1.0, step=0.01, value=train_x_sorted[-1][0], layout=Layout(width='500px')))\n",
    "        f(ax[0][0], train_x_sorted[-1][0], predictive_mean)\n",
    "       # plt.savefig(f\"{DIR_NAME}/chi2_bestR_{HIST_INDEX}.png\")\n",
    "        #interact(g, x=FloatSlider(description=axis_titles[1], min=0.0, max=2*math.pi, step=0.01, value=train_x_sorted[-1][1], layout=Layout(width='500px')))\n",
    "        g(ax[0][1], train_x_sorted[-1][1], predictive_mean)\n",
    "       # plt.savefig(f\"{DIR_NAME}/chi2_bestPhi_{HIST_INDEX}.png\")\n",
    "        #interact(h, x=FloatSlider(description=axis_titles[2], min=0.0, max=2, step=0.01, value=train_x_sorted[-1][2], layout=Layout(width='500px')))\n",
    "        h(ax[0][2], train_x_sorted[-1][2], predictive_mean)\n",
    "       # plt.savefig(f\"{DIR_NAME}/chi2_bestZ_{HIST_INDEX}.png\")\n",
    "\n",
    "       # f(ax[2][0], train_x_sorted[-1][0], predictive_upper - predictive_lower)\n",
    "       # g(ax[2][1], train_x_sorted[-1][1], predictive_upper - predictive_lower)\n",
    "       # h(ax[2][2], train_x_sorted[-1][2], predictive_upper - predictive_lower)\n",
    "        \n",
    "        bestValues = train_x_sorted[-1]\n",
    "        lambda_theta, lambda_phi, lambda_theta_phi = lambdas(bestValues)\n",
    "        bestHistsMC = getHistMakerMC(HIST_INDEX).makeHists(lambda_theta, lambda_phi, lambda_theta_phi)\n",
    "       # bestIndex = int(c_sorted[-1][0].item())\n",
    "        can1, hdiff1, py1 = plotComparison(bestHistsMC[0][HIST_INDEX], histsData[0][HIST_INDEX])\n",
    "        can1.SaveAs(f\"{DIR_NAME}/comparison_bestIndex_{HIST_INDEX}.gif\")\n",
    "        can1.SaveAs(f\"{DIR_NAME}/comparison_bestIndex_{HIST_INDEX}.pdf\")\n",
    "\n",
    "        worstValues = train_x_sorted[0]\n",
    "        lambda_theta, lambda_phi, lambda_theta_phi = lambdas(worstValues)\n",
    "        worstHistsMC = getHistMakerMC(HIST_INDEX).makeHists(lambda_theta, lambda_phi, lambda_theta_phi)\n",
    "       # worstIndex = int(c_sorted[0][0].item())\n",
    "        can2, hdiff2, py2 = plotComparison(bestHistsMC[0][HIST_INDEX], histsData[0][HIST_INDEX])\n",
    "        can2.SaveAs(f\"{DIR_NAME}/comparison_worstIndex_{HIST_INDEX}.gif\")\n",
    "        can2.SaveAs(f\"{DIR_NAME}/comparison_worstIndex_{HIST_INDEX}.pdf\")\n",
    "\n",
    "        if bestValues[0] > 0.3:\n",
    "            diffToBest = torch.tensor([-0.3,0,0])\n",
    "        else:\n",
    "            diffToBest = torch.tensor([+0.3,0,0])\n",
    "        notSoGoodValues = bestValues + diffToBest\n",
    "        lambda_theta, lambda_phi, lambda_theta_phi = lambdas(notSoGoodValues)\n",
    "        notSoGoodHistsMC = getHistMakerMC(HIST_INDEX).makeHists(lambda_theta, lambda_phi, lambda_theta_phi)\n",
    "        can22, hdiff22, py22 = plotComparison(notSoGoodHistsMC[0][HIST_INDEX], histsData[0][HIST_INDEX])\n",
    "        can22.SaveAs(f\"{DIR_NAME}/comparison_notSOGoodValues_{HIST_INDEX}.gif\")\n",
    "        can22.SaveAs(f\"{DIR_NAME}/comparison_notSOGoodValues_{HIST_INDEX}.pdf\")\n",
    "\n",
    "        all = all_lambdas(xs)\n",
    "        cov_lambda = np.cov(all, rowvar=False, aweights=predictive_mean_2)\n",
    "        sgn_lambda = 1.0*(cov_lambda > 0) + -1.0*(cov_lambda < 0)\n",
    "\n",
    "        np.multiply(\n",
    "            sgn_lambda, np.sqrt(\n",
    "                np.multiply(\n",
    "                    sgn_lambda, cov_lambda\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        print(str(HIST_INDEX) + \": Final result:\")\n",
    "        print(str(HIST_INDEX) + \": Final result:\", file=fout)\n",
    "        print(str(HIST_INDEX) + \": lambdas = \", c_sorted[-1][1:4])\n",
    "        print(str(HIST_INDEX) + \": lambdas = \", c_sorted[-1][1:4], file=fout)\n",
    "   # print(str(HIST_INDEX) + \": covariance matrix: \\n\", cov_lambda)\n",
    "\n",
    "   # hpm = convert_to_h3d(predictive_mean, HIST_INDEX)\n",
    "   # covariance_fit(hpm,train_x_sorted[-1],HIST_INDEX)\n",
    "        drphiz = covariance_fit_scipy(predictive_mean,predictive_lower,predictive_upper,train_x_sorted[-1],HIST_INDEX,ax)\n",
    "\n",
    "        print(str(HIST_INDEX) + \": errors = \", lambda_errors(train_x_sorted[-1], drphiz))\n",
    "        print(str(HIST_INDEX) + \": errors = \", lambda_errors(train_x_sorted[-1], drphiz), file=fout)\n",
    "    \n",
    "#allHistsMC = []\n",
    "#for ind in range(1):\n",
    "#    task(ind)\n",
    "\n",
    "#with multiprocessing.Pool() as pool:\n",
    "#    pool.map(task, range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_upper-predictive_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictive_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
